articles: 100
completion_tokens: 53020
dataset_type: null
input_tokens: 53020
max_batch_size_dev: null
max_batch_size_train: 13
max_seq_length: 1024
number_of_dev_files: 0
number_of_test_files: 0
number_of_training_files: 4
output_tokens: 55296
padding_tokens: 2276
prompt_tokens: 0
sequences: 54
token_type_ids: true
tokenizer_model_type: "<class 'transformers.models.gpt2.configuration_gpt2.GPT2Config'>"
tokens_dropped_from_all_prompt: 0
tokens_dropped_from_packing: 0
vocab_size: 50257
